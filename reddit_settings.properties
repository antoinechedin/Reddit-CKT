# How to aggregate.
aggregation_type=null

# If type is 'weights', an array of weights applied to each metric, then a final constant to add.
# If type is 'script', the path to the script.
aggregation_value=[]

# Whether the metric scores should be centered and reduced in [0,1].
center_metrics=true

column_karma_predicted=karma_predicted

predictions_file=dnnr_predictions.csv

column_sequence=author

column_order=created_utc

column_problem=id

column_score=score

# Directory containing datasets to use.
dataset_directory=datasets/testset

# path to a script (from root) if executed, null else
execute_script=null

# Number of folds in the cross validation (=number of KTFIs).
fold_count=2

# Number of threshold values to test (=number of KTTIs).
threshold_count=20

# False if no knowledge tracing on the metrics. (metrics are only used to compute the score).
kt_on_metrics=false

# Null if no max
max_value=20

# Null if no min
min_value=0

# The list of thresholds to apply to the metrics. Required to apply Knowledge Tracing on each metric.
# If a threshold starts with '<', then the score needs to be inferior to the threshold to be valid.  
metric_threshold=[]
	
# The list of metrics.
# rating	reviewTextLength	summaryLength	reviewTextLengthSpellingErrorRatio	summarySpellingErrorRatio	reviewTextFOG	summaryFOG	reviewTextFK	summaryFK	reviewTextARI	summaryARI	reviewTextCLI	summaryCLI	polarityReviewText	polaritySummary	deviation
metrics=[]

# The path to the file to export KT parameters & model evaluation.
output_params=params.csv

# Directory to export results to.
results_directory=results/testset

# How to handle the input scores. 'compute' to use the metrics to calculate the score; 'reduce' to reduce and center the input scores; or 'nothing' to use the input scores as is.
scores=reduce

# If 'false', only the last problem of each sequence will be used for RMSE.
# If 'true', each problem from the closest to expected to the end will be used.
smooth_rmse=true
