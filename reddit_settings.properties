# How to aggregate.
aggregation_type=weights

# If type is 'weights', an array of weights applied to each metric, then a final constant to add.
# If type is 'script', the path to the script.
aggregation_value=[2.762,-3.4924,0.6588,-1.2901,-0.8216,-0.3086,-0.1656,10.2871,8.0863,0.335,0.535,-1.2221,-0.1556,0.6134,-7.424]

# Whether the metric scores should be centered and reduced in [0,1].
center_metrics=false

# true if the correctness is in the input file; else represents a threshold: 
# problems with scores superior than the threshold will be considered as correct.
correctness=3.2

# The part of sequences used as test set.
cross_validation=0.1

# false if expected knowledge and computed knowledge should be compared as is.
# Else, expected knowledge is binary; and computed knowledge will be compared to it using this threshold. 
expected_binary=false

# The relative path to the file containing input data.
input_file=results_with_feature_normalized.csv

# False if no knowledge tracing on the metrics. (metrics are only used to compute the score).
kt_on_metrics=true

# Null if no max
max_score=null

# Null if no min
min_score=0

# The list of thresholds to apply to the metrics. Required to apply Knowledge Tracing on each metric.
# If a threshold starts with '<', then the score needs to be inferior to the threshold to be valid.  
metric_threshold=[0.5,<0.5,0.5,<0.5,<0.5,<0.5,<0.5,0.5,0.5,0.5,<0.5,<0.5,0.5,<0.5]
	
# The list of metrics.
# rating	reviewTextLength	summaryLength	reviewTextLengthSpellingErrorRatio	summarySpellingErrorRatio	reviewTextFOG	summaryFOG	reviewTextFK	summaryFK	reviewTextARI	summaryARI	reviewTextCLI	summaryCLI	polarityReviewText	polaritySummary	deviation
metrics=[selftextLength,selftextARI,titleARI,selftextSpellingError,titleSpellingError,selftextSpellingErrorRatio,titleSpellingErrorRatio,selftextFOG,selftextFK,titleFK,selftextCLI,polaritySelftext,polarityTitle,polarity]

# The path to the file to export the computed parameters.
output_params=output-params-reddit.csv

# The path to the file to export the local KT parameters per metric.
output_local_KT=output-local-kt.csv

# The path to the file to export the knowledge found for each problem.
output_sequences=null

# The path to the file to export the knowledge found for each metric.
output_metrics=null

# The path to the file to export the data to make user graphs.
output_user_graphs=output-usergraphs-reddit.csv

# How to handle the input scores. 'compute' to use the metrics to calculate the score; 'reduce' to reduce and center the input scores; or 'nothing' to use the input scores as is.
scores=compute

# If 'false', only the last problem of each sequence will be used for RMSE.
# If 'true', each problem from the closest to expected to the end will be used.
smooth_rmse=true

# false if no split. Else, represents the maximum number of consecutive problems that can be considered as noise.
split=false
